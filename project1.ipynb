{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import lit, monotonically_increasing_id\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pandas\n",
    "# Reducing dataframe memory usage :-\n",
    "def ReduceMemory(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function reduces the associated dataframe's memory usage.\n",
    "    It reassigns the data-types of columns according to their min-max values.\n",
    "    It also displays the dataframe information after memory reduction.\n",
    "    \"\"\";\n",
    "    \n",
    "    # Reducing float column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('float').columns):\n",
    "        col_min = np.amin(df[col].dropna());\n",
    "        col_max = np.amax(df[col].dropna());\n",
    "        \n",
    "        if col_min >= np.finfo(np.float16).min and col_max <= np.finfo(np.float16).max: \n",
    "            df[col] = df[col].astype(np.float16)\n",
    "        elif col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max : \n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        else: pass;\n",
    "\n",
    "    # Reducing integer column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('int').columns):\n",
    "        col_min = df[col].min(); \n",
    "        col_max = df[col].max();\n",
    "        \n",
    "        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8);\n",
    "        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16);\n",
    "        elif col_min >= np.iinfo(np.int32).min & col_max <= np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32);\n",
    "        else: pass;\n",
    "        \n",
    "    display(df.info());\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dylan/repo/CMU18763_Projects1/project1.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdylanli3090/home/dylan/repo/CMU18763_Projects1/project1.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m appName \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mProject1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdylanli3090/home/dylan/repo/CMU18763_Projects1/project1.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m master \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdylanli3090/home/dylan/repo/CMU18763_Projects1/project1.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(appName)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdylanli3090/home/dylan/repo/CMU18763_Projects1/project1.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sqlContext \u001b[39m=\u001b[39m SQLContext(sc)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdylanli3090/home/dylan/repo/CMU18763_Projects1/project1.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m spark \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39msparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# init a spark session\n",
    "appName = \"Project1\"\n",
    "master = \"local\"\n",
    "\n",
    "\n",
    "sc = SparkSession.builder.appName(appName).getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fifa data folder should contain all the csv files from Fifa(Kaggle), 2015-2022\n",
    "# assume that you are working in the same directory as the data folder\n",
    "full_data_path = os.getcwd() + '/full_data.csv'\n",
    "\n",
    "if not os.path.exists(full_data_path):\n",
    "    data_path = os.getcwd() + '/fifadata'\n",
    "    if os.path.exists(data_path):\n",
    "        print(\"Data folder exists\")\n",
    "    else:\n",
    "        print(\"Data folder does not exist\")\n",
    "        os.makedirs(data_path)\n",
    "        print(\"Sussessfully created data folder\")\n",
    "\n",
    "    csv_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "    print(csv_files)\n",
    "    combined_df = None\n",
    "    for file in csv_files:\n",
    "        year = file.split(\"players_\")[1].split(\".csv\")[0]\n",
    "        df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "        df = df.withColumn(\"year\", lit(year)) # this is the unique column 'year'\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.union(df)\n",
    "    combined_df = combined_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "    # Write the concatenated DataFrame to a new CSV file\n",
    "    output_file = \"/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/full_data.csv\"\n",
    "    ReduceMemory(combined_df.toPandas()).to_csv(output_file)\n",
    "else: \n",
    "    df = spark.read.csv(full_data_path, header=True, inferSchema=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sofifa_id: integer (nullable = true)\n",
      " |-- player_url: string (nullable = true)\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- long_name: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: double (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: double (nullable = true)\n",
      " |-- club_loaned_from: string (nullable = true)\n",
      " |-- club_joined: date (nullable = true)\n",
      " |-- club_contract_valid_until: double (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- nation_team_id: double (nullable = true)\n",
      " |-- nation_position: string (nullable = true)\n",
      " |-- nation_jersey_number: double (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- real_face: string (nullable = true)\n",
      " |-- release_clause_eur: double (nullable = true)\n",
      " |-- player_tags: string (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: double (nullable = true)\n",
      " |-- shooting: double (nullable = true)\n",
      " |-- passing: double (nullable = true)\n",
      " |-- dribbling: double (nullable = true)\n",
      " |-- defending: double (nullable = true)\n",
      " |-- physic: double (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: integer (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- goalkeeping_speed: double (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- player_face_url: string (nullable = true)\n",
      " |-- club_logo_url: string (nullable = true)\n",
      " |-- club_flag_url: string (nullable = true)\n",
      " |-- nation_logo_url: string (nullable = true)\n",
      " |-- nation_flag_url: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read table and write to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"010323\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "    \n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Find X clubs that have the highest number of players with contracts ending in 2023\n",
    "\n",
    "We find that En Avant de Guingamp have the most players whose contract end in 2023 in FIFA 2022 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1 = spark.sql(\"\"\"\n",
    "SELECT dv.club_name, COUNT(*) AS player_count\n",
    "FROM   df_view dv\n",
    "WHERE  dv.year = 22 AND dv.club_contract_valid_until = 2023\n",
    "GROUP BY dv.club_name\n",
    "ORDER BY player_count DESC\n",
    "LIMIT 10;\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In original pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def find_top_clubs_expiring_contracts(df, num_clubs):\n",
    "\n",
    "  top_clubs = (df.filter((F.col('year') == '22') & \n",
    "                        (F.col('club_contract_valid_until') == 2023))\n",
    "               .groupBy('club_team_id', 'club_name')\n",
    "               .agg(F.count('sofifa_id').alias('num_players'))\n",
    "               .orderBy(F.desc('num_players'))\n",
    "               .limit(num_clubs))\n",
    "               \n",
    "  top_names = [row.club_name for row in top_clubs.collect()]\n",
    "  \n",
    "  return top_clubs, top_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs, names = find_top_clubs_expiring_contracts(df_read, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En Avant de Guingamp', 'Club Atlético Lanús', 'Lechia Gdańsk', 'Kasimpaşa SK', 'Barnsley', 'Bengaluru FC', 'FC Barcelona', 'Zagłębie Lubin', 'SV Wehen Wiesbaden', 'KAA Gent'] Club with highest number of players with contracts ending in 2023 in FIFA 2022\n"
     ]
    }
   ],
   "source": [
    "print(f\"{names} Club with highest number of players with contracts ending in 2023 in FIFA 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|club_team_id|           club_name|num_players|\n",
      "+------------+--------------------+-----------+\n",
      "|        62.0|En Avant de Guingamp|         19|\n",
      "|    110395.0| Club Atlético Lanús|         17|\n",
      "|    111091.0|       Lechia Gdańsk|         17|\n",
      "|    111339.0|        Kasimpaşa SK|         16|\n",
      "|      1932.0|            Barnsley|         16|\n",
      "|    113302.0|        Bengaluru FC|         16|\n",
      "|       241.0|        FC Barcelona|         15|\n",
      "|    110749.0|      Zagłębie Lubin|         15|\n",
      "|       492.0|  SV Wehen Wiesbaden|         15|\n",
      "|       674.0|            KAA Gent|         15|\n",
      "+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clubs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2 = spark.sql(\"\"\"\n",
    "WITH club_counts AS (\n",
    "  SELECT \n",
    "    dv.club_name, \n",
    "    dv.year, \n",
    "    COUNT(*) AS player_count\n",
    "  FROM df_view dv\n",
    "  WHERE dv.age > 27\n",
    "  GROUP BY dv.club_name, dv.year\n",
    "),\n",
    "\n",
    "club_averages AS (\n",
    "  SELECT \n",
    "    cc.club_name,\n",
    "    AVG(cc.player_count) AS avg_player_count\n",
    "  FROM club_counts cc\n",
    "  GROUP BY cc.club_name  \n",
    ")\n",
    "\n",
    "SELECT\n",
    "  ca.club_name,\n",
    "  ca.avg_player_count AS average_count\n",
    "FROM club_averages ca\n",
    "WHERE (\n",
    "  SELECT COUNT(*) \n",
    "  FROM club_averages ca2 \n",
    "  WHERE ca2.avg_player_count > ca.avg_player_count\n",
    ") < 10\n",
    "ORDER BY ca.avg_player_count DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In original pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def find_top_clubs_by_avg_older_players(df, num_clubs):\n",
    "\n",
    "  window = Window.orderBy(F.desc('avg_count'))\n",
    "  df = df.repartition(100)\n",
    "  top_clubs = (df.filter(F.col('age') > 27)\n",
    "               .groupBy('year', 'club_team_id', 'club_name')\n",
    "               .agg(F.count('sofifa_id').alias('count'))\n",
    "               .groupBy('club_team_id', 'club_name')\n",
    "               .agg(F.avg('count').alias('avg_count'))\n",
    "               .withColumn('rank', F.rank().over(window))\n",
    "               .filter(F.col('rank') <= num_clubs)\n",
    "               .select('club_team_id', 'club_name'))\n",
    "               \n",
    "  top_names = [row.club_name for row in top_clubs.collect()]\n",
    "\n",
    "\n",
    "\n",
    "  return top_clubs, top_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:51:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clubs, names = find_top_clubs_by_avg_older_players(df_read, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:51:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 197:=========================================>           (79 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|club_team_id|           club_name|\n",
      "+------------+--------------------+\n",
      "|        null|                null|\n",
      "|    111032.0|  Dorados de Sinaloa|\n",
      "|    113158.0| Matsumoto Yamaga FC|\n",
      "|    110955.0| Shanghai Shenhua FC|\n",
      "|    112961.0|          Qingdao FC|\n",
      "|    110974.0|Club Deportivo Jo...|\n",
      "|    101006.0|            Altay SK|\n",
      "|    114688.0|         Guaireña FC|\n",
      "|    101014.0|İstanbul Başakşeh...|\n",
      "|    114511.0|      Sport Huancayo|\n",
      "|       749.0|      BB Erzurumspor|\n",
      "|    101108.0|        Club Olimpia|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/12 20:51:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clubs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'Dorados de Sinaloa', 'Matsumoto Yamaga FC', 'Shanghai Shenhua FC', 'Qingdao FC', 'Club Deportivo Jorge Wilstermann', 'Altay SK', 'Guaireña FC', 'İstanbul Başakşehir FK', 'Sport Huancayo', 'BB Erzurumspor', 'Club Olimpia'] clubs with highest avg players over 27:\n"
     ]
    }
   ],
   "source": [
    "print(f\"{names} clubs with highest avg players over 27:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3 = spark.sql(\"\"\"\n",
    "WITH yearly_counts AS (\n",
    "    SELECT dv.year, dv.nation_position, COUNT(*) AS position_count\n",
    "    FROM df_view dv\n",
    "    WHERE dv.nation_position IS NOT NULL\n",
    "    GROUP BY dv.year, dv.nation_position\n",
    "),\n",
    "max_counts AS (\n",
    "    SELECT yc.year, MAX(yc.position_count) AS max_count\n",
    "    FROM yearly_counts yc\n",
    "    GROUP BY yc.year\n",
    ")\n",
    "SELECT mc.year, yc.nation_position, mc.max_count\n",
    "FROM max_counts mc\n",
    "JOIN yearly_counts yc ON mc.year = yc.year AND mc.max_count = yc.position_count\n",
    "ORDER BY mc.year;\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In original pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def most_frequent_nation_position(df):\n",
    "\n",
    "  window = Window.partitionBy('year').orderBy(F.desc('count'))\n",
    "\n",
    "  top_positions = (df.filter(F.col('nation_position').isNotNull())\n",
    "                  .groupBy('year', 'nation_position')\n",
    "                  .agg(F.count('sofifa_id').alias('count'))\n",
    "                  .withColumn('rank', F.rank().over(window))\n",
    "                  .filter(F.col('rank') == 1)\n",
    "                  .select('year', 'nation_position'))\n",
    "\n",
    "  return top_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|year|nation_position|\n",
      "+----+---------------+\n",
      "|  15|            SUB|\n",
      "|  16|            SUB|\n",
      "|  17|            SUB|\n",
      "|  18|            SUB|\n",
      "|  19|            SUB|\n",
      "|  20|            SUB|\n",
      "|  21|            SUB|\n",
      "|  22|            SUB|\n",
      "+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_frequent_nation_position(df_read).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Useless column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless column\n",
    "useless_columns = ['sofifa_id', 'player_url', 'long_name', 'dob', 'club_loaned_from',\n",
    "                   'nation_position', 'nation_jersey_number', 'body_type', 'real_face',\n",
    "                   'player_face_url', 'club_logo_url', 'nation_logo_url', 'nation_flag_url', 'club_flag_url',\n",
    "                    'goalkeeping_speed', 'player_tags', 'nation_team_id', 'short_name', 'league_name','id', 'club_joined','club_contract_valid_until'] #TODO how to deal with time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_df = df_read.drop(*useless_columns)\n",
    "new_df = df.drop(*useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_level: double (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: double (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- release_clause_eur: double (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: double (nullable = true)\n",
      " |-- shooting: double (nullable = true)\n",
      " |-- passing: double (nullable = true)\n",
      " |-- dribbling: double (nullable = true)\n",
      " |-- defending: double (nullable = true)\n",
      " |-- physic: double (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: integer (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete columns that include url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get a list of columns that include URLs\n",
    "url_columns = [c for c in new_df.columns if 'url' in c]\n",
    "\n",
    "# Drop the columns that include URLs\n",
    "new_df = new_df.drop(*url_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Columns that Missing Value are more than 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in new_df.columns:\n",
    "    missing = new_df.filter(col(i).isNull()).count() / new_df.count() * 100\n",
    "    if missing > 50:\n",
    "        print('{} - {}%'.format(i, round(missing)))\n",
    "        cols_to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop value after +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "columns1 = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram',\n",
    "            'lm','lcm','cm','rcm','rm','lwb','ldm', 'cdm','rdm','rwb',\n",
    "            'lb','lcb','cb','rcb','rb']\n",
    "\n",
    "for col in columns1:\n",
    "    new_df = new_df.withColumn(col, split(new_df[col], '\\+').getItem(0).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_value = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [c for c, t in new_df.dtypes if t == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in string_cols:\n",
    "  new_df = new_df.fillna(na_value, subset=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, when, col, array_contains\n",
    "import itertools\n",
    "\n",
    "# Split positions into array\n",
    "split_positions = split(new_df['player_positions'], ', ')  \n",
    "\n",
    "# Get distinct positions as a list\n",
    "distinct_positions = list(set(list(itertools.chain(*new_df.select(split_positions.alias('positions')).distinct().rdd.flatMap(lambda x: x).collect()))))\n",
    "\n",
    "# Create a column for each distinct position\n",
    "for position in distinct_positions:\n",
    "  new_df = new_df.withColumn(\n",
    "    'Position_' + position,\n",
    "     when(array_contains(split_positions, position), 1).otherwise(0)\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop('player_positions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values again\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])\n",
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna preprocess the preffered_foot using one-hot encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "indexer = StringIndexer(inputCol='preferred_foot', outputCol='indexed_preferred_foot')\n",
    "encoder = OneHotEncoder(inputCols=['indexed_preferred_foot'], outputCols=['preferred_foot_encoded'])\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer,encoder])\n",
    "\n",
    "model = pipeline.fit(new_df)\n",
    "\n",
    "col_to_drop = ['indexed_preferred_foot','preferred_foot']\n",
    "data_encoded = model.transform(new_df).drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder for work_rate and player_positions label_encoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Loop over each string column in the DataFrame\n",
    "for col_name, data_type in data_encoded.dtypes:\n",
    "    if data_type == 'string':\n",
    "        # Create a StringIndexer object and fit it to the column\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=col_name + '_indexed')\n",
    "        model = indexer.fit(data_encoded)\n",
    "        \n",
    "        # Transform the column using the fitted indexer\n",
    "        data_encoded = model.transform(data_encoded).drop(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using A Denosing Autoencoder to Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_columns(df):\n",
    "    result = pd.DataFrame({\n",
    "        'unique': df.nunique() == len(df),\n",
    "        'cardinality': df.nunique(),\n",
    "        'with_null': df.isna().any(),\n",
    "        'null_pct': round((df.isnull().sum() / len(df)) * 100, 2),\n",
    "        'zero_count': (df == 0).sum(),\n",
    "        'mean': df.mean(),\n",
    "        'median': df.median(),\n",
    "        'std_dev': df.std(),\n",
    "        'min': df.min(),\n",
    "        'max': df.max(),\n",
    "        '1st_quantile': df.quantile(0.25),\n",
    "        '2nd_quantile': df.quantile(0.50),\n",
    "        '3rd_quantile': df.quantile(0.75),\n",
    "        '1st_row': df.iloc[0],\n",
    "        'random_row': df.iloc[np.random.randint(low=0, high=len(df))],\n",
    "        'last_row': df.iloc[-1],\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = data_encoded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data\n",
    "pd_df.to_csv('/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/pypark_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noencoded = pd_df.iloc[:,:-6]\n",
    "data_noencoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noencoded = ReduceMemory(data_noencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_columns(data_noencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masks\n",
    "\n",
    "\n",
    "# Mask for training -> binomial + base one per row\n",
    "def random_mask(shape, binomial_p=0.05):\n",
    "    n, k = shape\n",
    "    mask = np.ones((n, k))\n",
    "    # Set minimum one random per row\n",
    "    mask[(\n",
    "        np.arange(n),\n",
    "        np.random.randint(0, k, n)\n",
    "    )] = 0\n",
    "    # Add binomial probability as well\n",
    "    b_mask = np.random.binomial(1, 1-binomial_p, (n, k))\n",
    "    return mask * b_mask\n",
    "\n",
    "\n",
    "# Mask for validation - fixed n missing per row\n",
    "def mask_n_rows(shape, n_missing):\n",
    "    n, k = shape\n",
    "    s = np.arange(k)[np.newaxis, :].repeat(n, axis=0).reshape(n, k)\n",
    "    idx = np.random.rand(n, k).argsort(1)[:,:n_missing]\n",
    "    col_idx = np.take_along_axis(s, idx, axis=1).ravel()\n",
    "    row_idx = np.arange(n).repeat(n_missing)\n",
    "    \n",
    "    mask = np.ones((n, k))\n",
    "    mask[(\n",
    "        row_idx,\n",
    "        col_idx\n",
    "    )] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_noencoded['overall']\n",
    "features = data_noencoded.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train and validation set\n",
    "\n",
    "is_missing_bool = data.loc[:,:].isna().sum(axis=1) > 0\n",
    "\n",
    "# Define subsets of the data with row-wise missing values\n",
    "X_complete = data.loc[~is_missing_bool, :].values\n",
    "X_missing = data.loc[is_missing_bool, :].values\n",
    "\n",
    "# Split data that has no missing to use for eval set\n",
    "X_train_complete, X_valid = train_test_split(X_complete, random_state=6) # Same as previous\n",
    "\n",
    "# Build train set from complete and missing data\n",
    "X_train = np.concatenate([X_train_complete, X_missing], axis=0)\n",
    "\n",
    "# Mask to show train values that have been imputed\n",
    "srce_nan_train = np.concatenate([\n",
    "    np.zeros(X_train_complete.shape),\n",
    "    data.loc[is_missing_bool, :].isna().astype(np.uint8).values\n",
    "])\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(data.loc[:,:].values)\n",
    "\n",
    "X_train = np.nan_to_num(scaler.transform(X_train), 0.0)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "# Dense layer with layer normalization and mish activation\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.act = nn.Mish()\n",
    "        self.layernorm = nn.LayerNorm(output_size, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.act(x)\n",
    "        return self.layernorm(x)\n",
    "    \n",
    "# Msked autoencoder model\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, n_columns, emb_dim,\n",
    "                 units=[512, 512, 512, 512, 512, 128]):\n",
    "        super().__init__()\n",
    "        self.n_columns = n_columns\n",
    "\n",
    "        # Embedding\n",
    "        self.inp_proj = nn.Linear(1, emb_dim)\n",
    "        self.mask_proj = nn.Linear(1, emb_dim)\n",
    "        self.emb_norm = nn.LayerNorm(n_columns * emb_dim, eps=1e-6)\n",
    "        \n",
    "        # MLP with skip connection\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(len(units)):\n",
    "            if i==0:\n",
    "                input_size = n_columns * emb_dim\n",
    "            elif i==1:\n",
    "                input_size = n_columns * emb_dim + units[0]\n",
    "            else:\n",
    "                input_size = units[i-1] + units[i-2]\n",
    "            output_size = units[i]\n",
    "            self.mlp_layers.append(\n",
    "                MLP(input_size=input_size, output_size=output_size)\n",
    "            )\n",
    "                \n",
    "        self.final_dense = nn.Linear(units[-1] + units[-2], self.n_columns)\n",
    "        \n",
    "    def forward(self, inputs:torch.Tensor, mask:torch.Tensor):\n",
    "        # Embeddings\n",
    "        input_embedding = self.inp_proj(torch.unsqueeze(inputs, 2))\n",
    "        mask_embedding = self.mask_proj(torch.unsqueeze(1-mask, 2))\n",
    "        embedding = input_embedding + mask_embedding\n",
    "        embedding = torch.flatten(embedding, 1)\n",
    "        x = [self.emb_norm(embedding)]\n",
    "        \n",
    "        # MLP\n",
    "        for i in range(len(self.mlp_layers)):\n",
    "            if i==0:\n",
    "                z = self.mlp_layers[i](x[0])\n",
    "                x.append(z)\n",
    "            else:\n",
    "                z = torch.cat((x[-1], x[-2]), 1)\n",
    "                z = self.mlp_layers[i](z)\n",
    "                x.append(z)\n",
    "                \n",
    "        x = torch.cat((x[-1], x[-2]), 1)\n",
    "        x = self.final_dense(x)\n",
    "        \n",
    "        # Output modification - predict only masked values, otherwise use inputs\n",
    "        outputs = torch.mul(inputs, mask) + torch.mul(1-mask, x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper validation method\n",
    "def validate(model, valid_mask, batch_size=4096):\n",
    "    assert valid_mask.shape == X_valid.shape\n",
    "    \n",
    "    n_batches_valid = X_valid.shape[0] // batch_size + 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ps = []\n",
    "        for batch in range(n_batches_valid):\n",
    "            x = torch.tensor(X_valid[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            mask = torch.tensor(valid_mask[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            x_masked = x * mask\n",
    "\n",
    "            p = model(x_masked, mask).cpu().numpy()\n",
    "            ps.append(p)\n",
    "\n",
    "        p = np.vstack(ps)\n",
    "        mask_bool = (1 - valid_mask).astype(bool)\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            scaler.inverse_transform(p)[mask_bool],\n",
    "            scaler.inverse_transform(X_valid)[mask_bool]\n",
    "        ))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to mask NaNs in the original data\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    # Mask should be 1 for masked value, 0 for unmasked value \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, inputs, target, mask):\n",
    "        loss = self.loss(inputs, target)\n",
    "        return torch.mean(loss * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters and learning rate schedule\n",
    "\n",
    "EPOCHS = 300\n",
    "LR_START = 0.001\n",
    "LR_END = 0.00005\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "# This cosine decay function is borrowed from AmbrosM in last month's TPS\n",
    "def cosine_decay(epoch):\n",
    "    epochs = EPOCHS\n",
    "    lr_start = LR_START\n",
    "    lr_end = LR_END\n",
    "    if epochs > 1:\n",
    "        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "    else:\n",
    "        w = 1\n",
    "    return w * lr_start + (1 - w) * lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# Build model\n",
    "device = 'mps'\n",
    "\n",
    "# params\n",
    "cols_n = X_train.shape[1]\n",
    "\n",
    "# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\n",
    "model = MaskedAutoencoder(cols_n, 15,units=[512, 512, 512, 512, 512, 256, 128]).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_decay)\n",
    "loss_fn = MaskedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# for epoch in epochs...\n",
    "\n",
    "np.random.seed(6)\n",
    "\n",
    "n = X_train.shape[0]\n",
    "\n",
    "batch_size = 4096\n",
    "n_batches = n // batch_size + 1\n",
    "index = np.arange(n)\n",
    "\n",
    "valid_per = 5\n",
    "\n",
    "# Validation Mask\n",
    "validation_masks = [mask_n_rows(X_valid.shape, i+1) for i in range(5)]\n",
    "validation_prob = list(data.loc[:,:].isna().sum(axis=1).value_counts() \\\n",
    "    / data.loc[data.loc[:,:].isna().sum(axis=1)>0, :].isna().sum(axis=1).value_counts().sum())[1:]\n",
    "\n",
    "c_scores = [np.zeros(EPOCHS) for i in range(len(validation_masks))]\n",
    "f_scores = np.zeros(EPOCHS)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} LR {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    losses = 0\n",
    "    norm_losses = 0\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_idx = index[i*batch_size:(i+1)*batch_size]\n",
    "        # Create batch train data\n",
    "        srce_mask = torch.tensor(srce_nan_train[batch_idx].astype(np.float32)).to(device)\n",
    "        x = torch.tensor(X_train[batch_idx].astype(np.float32)).to(device)\n",
    "        mask_init = torch.tensor(random_mask(x.shape, binomial_p=0.05).astype(np.float32)).to(device)\n",
    "        mask = mask_init - srce_mask * mask_init\n",
    "        x_masked = x * mask\n",
    "\n",
    "        # Forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x_masked, mask)\n",
    "        loss = loss_fn(p, x, srce_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss # Check\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    # Validation stepb\n",
    "    if (epoch + 1) % valid_per == 0:\n",
    "        scores = []\n",
    "        for i in range(len(validation_masks)):\n",
    "            v = validate(model, validation_masks[i])\n",
    "            scores.append(v)\n",
    "            c_scores[i][epoch] = v\n",
    "            \n",
    "        final_score = math.sqrt(sum([scores[i]**2 * validation_prob[i] for i in range(len(scores))]))\n",
    "        f_scores[epoch] = final_score\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            print(f'RMSE ({i+1} rows) {scores[i]}')\n",
    "        print(f'RMSE (TDGP) {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aichaintools-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import lit, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pandas\n",
    "# Reducing dataframe memory usage :-\n",
    "def ReduceMemory(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function reduces the associated dataframe's memory usage.\n",
    "    It reassigns the data-types of columns according to their min-max values.\n",
    "    It also displays the dataframe information after memory reduction.\n",
    "    \"\"\";\n",
    "    \n",
    "    # Reducing float column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('float').columns):\n",
    "        col_min = np.amin(df[col].dropna());\n",
    "        col_max = np.amax(df[col].dropna());\n",
    "        \n",
    "        if col_min >= np.finfo(np.float16).min and col_max <= np.finfo(np.float16).max: \n",
    "            df[col] = df[col].astype(np.float16)\n",
    "        elif col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max : \n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        else: pass;\n",
    "\n",
    "    # Reducing integer column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('int').columns):\n",
    "        col_min = df[col].min(); \n",
    "        col_max = df[col].max();\n",
    "        \n",
    "        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8);\n",
    "        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16);\n",
    "        elif col_min >= np.iinfo(np.int32).min & col_max <= np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32);\n",
    "        else: pass;\n",
    "        \n",
    "    display(df.info());\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"Project1\"\n",
    "master = \"local\"\n",
    "\n",
    "\n",
    "sc = SparkSession.builder.appName(appName).getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:> (5 + 25) / 30][Stage 17:>   (0 + 1) / 1][Stage 18:>   (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# fifa data folder should contain all the csv files from Fifa(Kaggle), 2015-2022\n",
    "# assume that you are working in the same directory as the data folder\n",
    "full_data_path = os.getcwd() + '/full_data.csv'\n",
    "\n",
    "if not os.path.exists(full_data_path):\n",
    "    data_path = os.getcwd() + '/fifadata'\n",
    "    if os.path.exists(data_path):\n",
    "        print(\"Data folder exists\")\n",
    "    else:\n",
    "        print(\"Data folder does not exist\")\n",
    "        os.makedirs(data_path)\n",
    "        print(\"Sussessfully created data folder\")\n",
    "\n",
    "    csv_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "    print(csv_files)\n",
    "    combined_df = None\n",
    "    for file in csv_files:\n",
    "        year = file.split(\"players_\")[1].split(\".csv\")[0]\n",
    "        df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "        df = df.withColumn(\"year\", lit(year)) # this is the unique column 'year'\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.union(df)\n",
    "    combined_df = combined_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "    # Write the concatenated DataFrame to a new CSV file\n",
    "    output_file = \"/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/full_data.csv\"\n",
    "    ReduceMemory(combined_df.toPandas()).to_csv(output_file)\n",
    "else: \n",
    "    df = spark.read.csv(full_data_path, header=True, inferSchema=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read table and write to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read multiple csv files into spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"010323\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "    \n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1 = spark.sql(\"\"\"\n",
    "SELECT dv.club_name, COUNT(*) AS player_count\n",
    "FROM   df_view dv\n",
    "WHERE  dv.year = 22 AND dv.club_contract_valid_until = 2023\n",
    "GROUP BY dv.club_name\n",
    "ORDER BY player_count DESC\n",
    "LIMIT 10;\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2 = spark.sql(\"\"\"\n",
    "WITH club_counts AS (\n",
    "    SELECT dv.club_name, dv.year, COUNT(*) AS player_count\n",
    "    FROM df_view dv\n",
    "    WHERE dv.age > 27\n",
    "    GROUP BY dv.club_name, dv.year\n",
    "),\n",
    "club_averages AS (\n",
    "    SELECT cc.club_name, AVG(cc.player_count) AS average_count\n",
    "    FROM club_counts cc\n",
    "    GROUP BY cc.club_name\n",
    ")\n",
    "SELECT ca.club_name, ca.average_count\n",
    "FROM club_averages ca\n",
    "WHERE (\n",
    "    SELECT COUNT(*) FROM club_averages ca2 WHERE ca2.average_count > ca.average_count\n",
    ") < 10\n",
    "ORDER BY ca.average_count DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3 = spark.sql(\"\"\"\n",
    "WITH yearly_counts AS (\n",
    "    SELECT dv.year, dv.nation_position, COUNT(*) AS position_count\n",
    "    FROM df_view dv\n",
    "    WHERE dv.nation_position IS NOT NULL\n",
    "    GROUP BY dv.year, dv.nation_position\n",
    "),\n",
    "max_counts AS (\n",
    "    SELECT yc.year, MAX(yc.position_count) AS max_count\n",
    "    FROM yearly_counts yc\n",
    "    GROUP BY yc.year\n",
    ")\n",
    "SELECT mc.year, yc.nation_position, mc.max_count\n",
    "FROM max_counts mc\n",
    "JOIN yearly_counts yc ON mc.year = yc.year AND mc.max_count = yc.position_count\n",
    "ORDER BY mc.year;\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 nad 4 are still processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Useless column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless column\n",
    "useless_columns = ['sofifa_id', 'player_url', 'long_name', 'dob', 'club_loaned_from',\n",
    "                   'nation_position', 'nation_jersey_number', 'body_type', 'real_face',\n",
    "                   'player_face_url', 'club_logo_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                    'goalkeeping_speed', 'player_tags', 'nation_team_id', 'short_name', 'league_name','id', 'club_joined','club_contract_valid_until'] #TODO how to deal with time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in linux use\n",
    "# df_read = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = df_read.drop(*useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete columns that include url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get a list of columns that include URLs\n",
    "url_columns = [c for c in new_df.columns if 'url' in c]\n",
    "\n",
    "# Drop the columns that include URLs\n",
    "new_df = new_df.drop(*url_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Columns that Missing Value are more than 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in new_df.columns:\n",
    "    missing = new_df.filter(col(i).isNull()).count() / new_df.count() * 100\n",
    "    if missing > 50:\n",
    "        print('{} - {}%'.format(i, round(missing)))\n",
    "        cols_to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop value after +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "columns1 = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram',\n",
    "            'lm','lcm','cm','rcm','rm','lwb','ldm', 'cdm','rdm','rwb',\n",
    "            'lb','lcb','cb','rcb','rb']\n",
    "\n",
    "for col in columns1:\n",
    "    new_df = new_df.withColumn(col, split(new_df[col], '\\+').getItem(0).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_value = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [c for c, t in new_df.dtypes if t == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in string_cols:\n",
    "  new_df = new_df.fillna(na_value, subset=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, when, col, array_contains\n",
    "import itertools\n",
    "\n",
    "# Split positions into array\n",
    "split_positions = split(new_df['player_positions'], ', ')  \n",
    "\n",
    "# Get distinct positions as a list\n",
    "distinct_positions = list(set(list(itertools.chain(*new_df.select(split_positions.alias('positions')).distinct().rdd.flatMap(lambda x: x).collect()))))\n",
    "\n",
    "# Create a column for each distinct position\n",
    "for position in distinct_positions:\n",
    "  new_df = new_df.withColumn(\n",
    "    'Position_' + position,\n",
    "     when(array_contains(split_positions, position), 1).otherwise(0)\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop('player_positions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values again\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])\n",
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna preprocess the preffered_foot using one-hot encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "indexer = StringIndexer(inputCol='preferred_foot', outputCol='indexed_preferred_foot')\n",
    "encoder = OneHotEncoder(inputCols=['indexed_preferred_foot'], outputCols=['preferred_foot_encoded'])\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer,encoder])\n",
    "\n",
    "model = pipeline.fit(new_df)\n",
    "\n",
    "col_to_drop = ['indexed_preferred_foot','preferred_foot']\n",
    "data_encoded = model.transform(new_df).drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder for work_rate and player_positions label_encoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Loop over each string column in the DataFrame\n",
    "for col_name, data_type in data_encoded.dtypes:\n",
    "    if data_type == 'string':\n",
    "        # Create a StringIndexer object and fit it to the column\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=col_name + '_indexed')\n",
    "        model = indexer.fit(data_encoded)\n",
    "        \n",
    "        # Transform the column using the fitted indexer\n",
    "        data_encoded = model.transform(data_encoded).drop(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using A Denosing Autoencoder to Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_columns(df):\n",
    "    result = pd.DataFrame({\n",
    "        'unique': df.nunique() == len(df),\n",
    "        'cardinality': df.nunique(),\n",
    "        'with_null': df.isna().any(),\n",
    "        'null_pct': round((df.isnull().sum() / len(df)) * 100, 2),\n",
    "        'zero_count': (df == 0).sum(),\n",
    "        'mean': df.mean(),\n",
    "        'median': df.median(),\n",
    "        'std_dev': df.std(),\n",
    "        'min': df.min(),\n",
    "        'max': df.max(),\n",
    "        '1st_quantile': df.quantile(0.25),\n",
    "        '2nd_quantile': df.quantile(0.50),\n",
    "        '3rd_quantile': df.quantile(0.75),\n",
    "        '1st_row': df.iloc[0],\n",
    "        'random_row': df.iloc[np.random.randint(low=0, high=len(df))],\n",
    "        'last_row': df.iloc[-1],\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = data_encoded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data\n",
    "pd_df.to_csv('/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/pypark_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noencoded = pd_df.iloc[:,:-6]\n",
    "data_noencoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noencoded = ReduceMemory(data_noencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_columns(data_noencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masks\n",
    "\n",
    "\n",
    "# Mask for training -> binomial + base one per row\n",
    "def random_mask(shape, binomial_p=0.05):\n",
    "    n, k = shape\n",
    "    mask = np.ones((n, k))\n",
    "    # Set minimum one random per row\n",
    "    mask[(\n",
    "        np.arange(n),\n",
    "        np.random.randint(0, k, n)\n",
    "    )] = 0\n",
    "    # Add binomial probability as well\n",
    "    b_mask = np.random.binomial(1, 1-binomial_p, (n, k))\n",
    "    return mask * b_mask\n",
    "\n",
    "\n",
    "# Mask for validation - fixed n missing per row\n",
    "def mask_n_rows(shape, n_missing):\n",
    "    n, k = shape\n",
    "    s = np.arange(k)[np.newaxis, :].repeat(n, axis=0).reshape(n, k)\n",
    "    idx = np.random.rand(n, k).argsort(1)[:,:n_missing]\n",
    "    col_idx = np.take_along_axis(s, idx, axis=1).ravel()\n",
    "    row_idx = np.arange(n).repeat(n_missing)\n",
    "    \n",
    "    mask = np.ones((n, k))\n",
    "    mask[(\n",
    "        row_idx,\n",
    "        col_idx\n",
    "    )] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_noencoded['overall']\n",
    "features = data_noencoded.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train and validation set\n",
    "\n",
    "is_missing_bool = data.loc[:,:].isna().sum(axis=1) > 0\n",
    "\n",
    "# Define subsets of the data with row-wise missing values\n",
    "X_complete = data.loc[~is_missing_bool, :].values\n",
    "X_missing = data.loc[is_missing_bool, :].values\n",
    "\n",
    "# Split data that has no missing to use for eval set\n",
    "X_train_complete, X_valid = train_test_split(X_complete, random_state=6) # Same as previous\n",
    "\n",
    "# Build train set from complete and missing data\n",
    "X_train = np.concatenate([X_train_complete, X_missing], axis=0)\n",
    "\n",
    "# Mask to show train values that have been imputed\n",
    "srce_nan_train = np.concatenate([\n",
    "    np.zeros(X_train_complete.shape),\n",
    "    data.loc[is_missing_bool, :].isna().astype(np.uint8).values\n",
    "])\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(data.loc[:,:].values)\n",
    "\n",
    "X_train = np.nan_to_num(scaler.transform(X_train), 0.0)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "# Dense layer with layer normalization and mish activation\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.act = nn.Mish()\n",
    "        self.layernorm = nn.LayerNorm(output_size, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.act(x)\n",
    "        return self.layernorm(x)\n",
    "    \n",
    "# Msked autoencoder model\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, n_columns, emb_dim,\n",
    "                 units=[512, 512, 512, 512, 512, 128]):\n",
    "        super().__init__()\n",
    "        self.n_columns = n_columns\n",
    "\n",
    "        # Embedding\n",
    "        self.inp_proj = nn.Linear(1, emb_dim)\n",
    "        self.mask_proj = nn.Linear(1, emb_dim)\n",
    "        self.emb_norm = nn.LayerNorm(n_columns * emb_dim, eps=1e-6)\n",
    "        \n",
    "        # MLP with skip connection\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(len(units)):\n",
    "            if i==0:\n",
    "                input_size = n_columns * emb_dim\n",
    "            elif i==1:\n",
    "                input_size = n_columns * emb_dim + units[0]\n",
    "            else:\n",
    "                input_size = units[i-1] + units[i-2]\n",
    "            output_size = units[i]\n",
    "            self.mlp_layers.append(\n",
    "                MLP(input_size=input_size, output_size=output_size)\n",
    "            )\n",
    "                \n",
    "        self.final_dense = nn.Linear(units[-1] + units[-2], self.n_columns)\n",
    "        \n",
    "    def forward(self, inputs:torch.Tensor, mask:torch.Tensor):\n",
    "        # Embeddings\n",
    "        input_embedding = self.inp_proj(torch.unsqueeze(inputs, 2))\n",
    "        mask_embedding = self.mask_proj(torch.unsqueeze(1-mask, 2))\n",
    "        embedding = input_embedding + mask_embedding\n",
    "        embedding = torch.flatten(embedding, 1)\n",
    "        x = [self.emb_norm(embedding)]\n",
    "        \n",
    "        # MLP\n",
    "        for i in range(len(self.mlp_layers)):\n",
    "            if i==0:\n",
    "                z = self.mlp_layers[i](x[0])\n",
    "                x.append(z)\n",
    "            else:\n",
    "                z = torch.cat((x[-1], x[-2]), 1)\n",
    "                z = self.mlp_layers[i](z)\n",
    "                x.append(z)\n",
    "                \n",
    "        x = torch.cat((x[-1], x[-2]), 1)\n",
    "        x = self.final_dense(x)\n",
    "        \n",
    "        # Output modification - predict only masked values, otherwise use inputs\n",
    "        outputs = torch.mul(inputs, mask) + torch.mul(1-mask, x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper validation method\n",
    "def validate(model, valid_mask, batch_size=4096):\n",
    "    assert valid_mask.shape == X_valid.shape\n",
    "    \n",
    "    n_batches_valid = X_valid.shape[0] // batch_size + 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ps = []\n",
    "        for batch in range(n_batches_valid):\n",
    "            x = torch.tensor(X_valid[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            mask = torch.tensor(valid_mask[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            x_masked = x * mask\n",
    "\n",
    "            p = model(x_masked, mask).cpu().numpy()\n",
    "            ps.append(p)\n",
    "\n",
    "        p = np.vstack(ps)\n",
    "        mask_bool = (1 - valid_mask).astype(bool)\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            scaler.inverse_transform(p)[mask_bool],\n",
    "            scaler.inverse_transform(X_valid)[mask_bool]\n",
    "        ))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to mask NaNs in the original data\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    # Mask should be 1 for masked value, 0 for unmasked value \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, inputs, target, mask):\n",
    "        loss = self.loss(inputs, target)\n",
    "        return torch.mean(loss * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters and learning rate schedule\n",
    "\n",
    "EPOCHS = 300\n",
    "LR_START = 0.001\n",
    "LR_END = 0.00005\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "# This cosine decay function is borrowed from AmbrosM in last month's TPS\n",
    "def cosine_decay(epoch):\n",
    "    epochs = EPOCHS\n",
    "    lr_start = LR_START\n",
    "    lr_end = LR_END\n",
    "    if epochs > 1:\n",
    "        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "    else:\n",
    "        w = 1\n",
    "    return w * lr_start + (1 - w) * lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# Build model\n",
    "device = 'mps'\n",
    "\n",
    "# params\n",
    "cols_n = X_train.shape[1]\n",
    "\n",
    "# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\n",
    "model = MaskedAutoencoder(cols_n, 15,units=[512, 512, 512, 512, 512, 256, 128]).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_decay)\n",
    "loss_fn = MaskedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# for epoch in epochs...\n",
    "\n",
    "np.random.seed(6)\n",
    "\n",
    "n = X_train.shape[0]\n",
    "\n",
    "batch_size = 4096\n",
    "n_batches = n // batch_size + 1\n",
    "index = np.arange(n)\n",
    "\n",
    "valid_per = 5\n",
    "\n",
    "# Validation Mask\n",
    "validation_masks = [mask_n_rows(X_valid.shape, i+1) for i in range(5)]\n",
    "validation_prob = list(data.loc[:,:].isna().sum(axis=1).value_counts() \\\n",
    "    / data.loc[data.loc[:,:].isna().sum(axis=1)>0, :].isna().sum(axis=1).value_counts().sum())[1:]\n",
    "\n",
    "c_scores = [np.zeros(EPOCHS) for i in range(len(validation_masks))]\n",
    "f_scores = np.zeros(EPOCHS)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} LR {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    losses = 0\n",
    "    norm_losses = 0\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_idx = index[i*batch_size:(i+1)*batch_size]\n",
    "        # Create batch train data\n",
    "        srce_mask = torch.tensor(srce_nan_train[batch_idx].astype(np.float32)).to(device)\n",
    "        x = torch.tensor(X_train[batch_idx].astype(np.float32)).to(device)\n",
    "        mask_init = torch.tensor(random_mask(x.shape, binomial_p=0.05).astype(np.float32)).to(device)\n",
    "        mask = mask_init - srce_mask * mask_init\n",
    "        x_masked = x * mask\n",
    "\n",
    "        # Forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x_masked, mask)\n",
    "        loss = loss_fn(p, x, srce_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss # Check\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    # Validation stepb\n",
    "    if (epoch + 1) % valid_per == 0:\n",
    "        scores = []\n",
    "        for i in range(len(validation_masks)):\n",
    "            v = validate(model, validation_masks[i])\n",
    "            scores.append(v)\n",
    "            c_scores[i][epoch] = v\n",
    "            \n",
    "        final_score = math.sqrt(sum([scores[i]**2 * validation_prob[i] for i in range(len(scores))]))\n",
    "        f_scores[epoch] = final_score\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            print(f'RMSE ({i+1} rows) {scores[i]}')\n",
    "        print(f'RMSE (TDGP) {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aichaintools-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "import pyspark.sql.functions as F \n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a spark session\n",
    "appName = \"Fifa_EDA\"\n",
    "master = \"local\"\n",
    "\n",
    "\n",
    "sc = SparkSession.builder.appName(appName).getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "data_path = '/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/full_data.csv'\n",
    "data = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that there  are some name and url columns that are not useful for our analysis. We will drop them.\n",
    "- We will drop `long_name`, because `short_name` and `sofifa_id` lacks representation information.\n",
    "- For dob and age, they provide same information. We keep age only for convenience.\n",
    "- For `club_team_id` and `club_name`. we keep `club_team_id` only. Because `club_team_id` is unique for each `club_name`, it is already a StringIndex col. We can use it directly for our analysis.\n",
    "- For `club_joined`, this is the date that the player joined the club. We will keep it then transfer it to years of joined and how many years the player has been in the club.\n",
    "- For `nationality_id` and `nationality_name`, we keep first one. \n",
    "- For `tags` and `player_traits`, consider split it to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if club_team_id and club_name is one to one. \n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def check_one_to_one(df: DataFrame, col1: str, col2: str):\n",
    "    # Group by col1 and count distinct values of col2\n",
    "    counts = df.groupBy(col1).agg(F.countDistinct(col2).alias('count'))\n",
    "\n",
    "    # Check if the maximum count is 1\n",
    "    max_count = counts.agg(F.max('count')).first()[0]\n",
    "    if max_count == 1: \n",
    "        return print(f'{col1} and {col2} is one by one.')\n",
    "    else: \n",
    "        return print(f'{col1} and {col2} is different. The max_diff_count is : {max_count}')\n",
    "\n",
    "check_one_to_one(data, 'club_team_id', 'club_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if sofifa_id and long_name is one to one.\n",
    "check_one_to_one(data, 'sofifa_id', 'long_name')\n",
    "check_one_to_one(data, 'short_name', 'long_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if national_id and national_name is one by one. \n",
    "check_one_to_one(data, 'nationality_id', 'nationality_name')\n",
    "check_one_to_one(data, 'nation_team_id', 'nation_position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def find_url_columns(df: DataFrame):\n",
    "    # Get a list of column names\n",
    "    column_names = df.columns \n",
    "    \n",
    "    url_columns = [col for col in column_names if 'url' in col]\n",
    "    \n",
    "    return url_columns \n",
    "\n",
    "print(f'The columns contain url are {find_url_columns(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify other unuseful columns\n",
    "useless_cols = ['player_url', 'player_face_url', 'club_logo_url', \n",
    "                'club_flag_url', 'nation_logo_url', 'nation_flag_url', 'sofifa_id', \n",
    "                'short_name', 'dob', 'club_name','club_jersey_number', 'club_loaned_from', \n",
    "                'nationality_name', 'nation_jersey_number', 'body_type','real_face', 'goalkeeping_speed', \n",
    "                'club_contract_valid_until']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we drop these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(*useless_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a clean datase, we can start our analysis. #TODO "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aichaintools-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"code","execution_count":1,"id":"90df1528","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: termcolor in /opt/conda/miniconda3/lib/python3.10/site-packages (2.3.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.10/site-packages (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pandas) (1.22.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.10/site-packages (4.66.1)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install termcolor\n","!pip install pandas \n","!pip install tqdm"]},{"cell_type":"markdown","id":"8d7c803f","metadata":{},"source":["### Utils "]},{"cell_type":"code","execution_count":2,"id":"a863ff4e","metadata":{},"outputs":[],"source":["# For Pyspark with No none\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import FloatType, IntegerType\n","from termcolor import colored\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from IPython.display import display\n","\n","def ReduceMemory(df: pd.DataFrame):\n","    \"\"\"\n","    This function reduces the associated dataframe's memory usage.\n","    It reassigns the data-types of columns according to their min-max values.\n","    It also displays the dataframe information after memory reduction.\n","    \"\"\";\n","    \n","    # Reducing float column memory usage:-\n","    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('float').columns):\n","        col_min = np.amin(df[col].dropna());\n","        col_max = np.amax(df[col].dropna());\n","        \n","        if col_min >= np.finfo(np.float16).min and col_max <= np.finfo(np.float16).max: \n","            df[col] = df[col].astype(np.float16)\n","        elif col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max : \n","            df[col] = df[col].astype(np.float32)\n","        else: pass;\n","\n","    # Reducing integer column memory usage:-\n","    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('int').columns):\n","        col_min = df[col].min(); \n","        col_max = df[col].max();\n","        \n","        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n","            df[col] = df[col].astype(np.int8);\n","        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n","            df[col] = df[col].astype(np.int16);\n","        elif col_min >= np.iinfo(np.int32).min & col_max <= np.iinfo(np.int32).max:\n","            df[col] = df[col].astype(np.int32);\n","        else: pass;\n","        \n","    print(colored(f\"\\nDataframe information after memory reduction\\n\", \n","                  color = 'blue', attrs= ['bold']));\n","    display(df.info()); \n","    \n","    return df;\n","    "]},{"cell_type":"markdown","id":"c74290c4","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":3,"id":"3fe227f3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://download.pytorch.org/whl/cpu\n","Requirement already satisfied: torch in /opt/conda/miniconda3/lib/python3.10/site-packages (2.1.1+cpu)\n","Requirement already satisfied: torchvision in /opt/conda/miniconda3/lib/python3.10/site-packages (0.16.1+cpu)\n","Requirement already satisfied: torchaudio in /opt/conda/miniconda3/lib/python3.10/site-packages (2.1.1+cpu)\n","Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (1.10.1)\n","Requirement already satisfied: networkx in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/miniconda3/lib/python3.10/site-packages (from torch) (2022.8.2)\n","Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.10/site-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.10/site-packages (from torchvision) (2.28.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from torchvision) (9.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"]},{"cell_type":"code","execution_count":4,"id":"ddddc841","metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math # For positional encoding in Transformer\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class MLPRegressor(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        super(MLPRegressor, self).__init__()\n","        layers = []\n","        for i in range(len(hidden_sizes)):\n","            if i == 0:\n","                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n","            else:\n","                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n","            layers.append(nn.ReLU())\n","\n","        self.layers = nn.Sequential(*layers)\n","        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","    \n","class ResidualBlock(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(ResidualBlock, self).__init__()\n","        self.fc = nn.Linear(input_size, output_size)\n","        self.activation = nn.ReLU()\n","\n","        # Adding a linear transformation for the residual link if input and output sizes differ\n","        if input_size != output_size:\n","            self.shortcut = nn.Linear(input_size, output_size)\n","        else:\n","            self.shortcut = nn.Identity()\n","\n","    def forward(self, x):\n","        identity = self.shortcut(x)\n","        out = self.fc(x)\n","        out = self.activation(out + identity)\n","        return out\n","\n","class MLPResidualRegressor(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        super(MLPResidualRegressor, self).__init__()\n","\n","        layers = []\n","        current_size = input_size\n","        for hidden_size in hidden_sizes:\n","            layers.append(ResidualBlock(current_size, hidden_size))\n","            current_size = hidden_size\n","\n","        self.layers = nn.Sequential(*layers)\n","        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.output_layer(x)\n","        return x\n","\n"]},{"cell_type":"markdown","id":"c42b229a","metadata":{},"source":["## Pipeline"]},{"cell_type":"code","execution_count":5,"id":"1807c06b","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import (\n","    Imputer,\n","    StandardScaler,\n","    StringIndexer,\n","    OneHotEncoder,\n","    VectorAssembler,\n",")\n","from pyspark.ml import Pipeline, Transformer\n","import os\n","from pyspark.sql.dataframe import DataFrame\n","from pyspark.sql.functions import lit, monotonically_increasing_id\n","from pyspark import SparkContext, SQLContext\n","from pyspark.sql import SparkSession\n","import pyspark\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import gc"]},{"cell_type":"code","execution_count":6,"id":"5567cecd","metadata":{},"outputs":[],"source":["# Config \n","SEED = 3407\n","IS_SPARKML = True\n","\n","\n","\n","def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","    \n","seed_everything(seed=SEED)"]},{"cell_type":"code","execution_count":7,"id":"dfa58af4","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/11/16 18:49:38 INFO SparkEnv: Registering MapOutputTracker\n","23/11/16 18:49:38 INFO SparkEnv: Registering BlockManagerMaster\n","23/11/16 18:49:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/11/16 18:49:38 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["spark = SparkSession.builder.appName(\"Task3\").getOrCreate()"]},{"cell_type":"code","execution_count":8,"id":"ae31d601","metadata":{},"outputs":[],"source":["# define some config\n","continuous_cols = [\n","    \"potential\",\n","    \"value_eur\",\n","    \"wage_eur\",\n","    \"age\",\n","    \"height_cm\",\n","    \"weight_kg\",\n","    \"club_team_id\",\n","    \"league_level\",\n","    \"nationality_id\",\n","    \"weak_foot\",\n","    \"skill_moves\",\n","    \"international_reputation\",\n","    \"pace\",\n","    \"shooting\",\n","    \"passing\",\n","    \"dribbling\",\n","    \"defending\",\n","    \"physic\",\n","    \"attacking_crossing\",\n","    \"attacking_finishing\",\n","    \"attacking_heading_accuracy\",\n","    \"attacking_short_passing\",\n","    \"attacking_volleys\",\n","    \"skill_dribbling\",\n","    \"skill_curve\",\n","    \"skill_fk_accuracy\",\n","    \"skill_long_passing\",\n","    \"skill_ball_control\",\n","    \"movement_acceleration\",\n","    \"movement_sprint_speed\",\n","    \"movement_agility\",\n","    \"movement_reactions\",\n","    \"movement_balance\",\n","    \"power_shot_power\",\n","    \"power_jumping\",\n","    \"power_stamina\",\n","    \"power_strength\",\n","    \"power_long_shots\",\n","    \"mentality_aggression\",\n","    \"mentality_interceptions\",\n","    \"mentality_positioning\",\n","    \"mentality_vision\",\n","    \"mentality_penalties\",\n","    \"defending_marking_awareness\",\n","    \"defending_standing_tackle\",\n","    \"defending_sliding_tackle\",\n","    \"goalkeeping_diving\",\n","    \"goalkeeping_handling\",\n","    \"goalkeeping_kicking\",\n","    \"goalkeeping_positioning\",\n","    \"goalkeeping_reflexes\",\n","    \"ls\",\n","    \"st\",\n","    \"rs\",\n","    \"lw\",\n","    \"lf\",\n","    \"cf\",\n","    \"rf\",\n","    \"rw\",\n","    \"lam\",\n","    \"cam\",\n","    \"ram\",\n","    \"lm\",\n","    \"lcm\",\n","    \"cm\",\n","    \"rcm\",\n","    \"rm\",\n","    \"lwb\",\n","    \"ldm\",\n","    \"cdm\",\n","    \"rdm\",\n","    \"rwb\",\n","    \"lb\",\n","    \"lcb\",\n","    \"cb\",\n","    \"rcb\",\n","    \"rb\",\n","    \"gk\",\n","    \"year\"\n","]\n","position_binary_cols = [\n","    \"Position_CF\",\n","    \"Position_LW\",\n","    \"Position_LM\",\n","    \"Position_RM\",\n","    \"Position_RW\",\n","    \"Position_ST\",\n","    \"Position_GK\",\n","    \"Position_CM\",\n","    \"Position_CDM\",\n","    \"Position_RB\",\n","    \"Position_CB\",\n","    \"Position_CAM\",\n","    \"Position_LB\",\n","    \"Position_RWB\",\n","    \"Position_LWB\",\n","]\n","nominal_cols = [\"league_name\", \"club_position\", \"work_rate\"]\n","# nominal_cols = [\"club_position\", \"work_rate\"]\n","ordinal_cols = [\"preferred_foot\"]\n","\n","cols_to_drop = [\n","    'id',\n","    'long_name',\n","    \"player_url\",\n","    \"player_face_url\",\n","    \"club_logo_url\",\n","    \"club_flag_url\",\n","    \"nation_logo_url\",\n","    \"nation_flag_url\",\n","    \"sofifa_id\",\n","    \"short_name\",\n","    \"dob\",\n","    \"club_name\",\n","    \"club_jersey_number\",\n","    \"club_loaned_from\",\n","    \"nationality_name\",\n","    \"nation_jersey_number\",\n","    \"body_type\",\n","    \"real_face\",\n","    \"goalkeeping_speed\",\n","    \"club_contract_valid_until\",\n","    \"nation_team_id\",\n","    \"nation_position\",\n","    \"player_tags\",\n","    \"player_traits\",\n","    \"release_clause_eur\",\n","    \"long_name\",\n","]\n"]},{"cell_type":"code","execution_count":9,"id":"fbf0df3b","metadata":{},"outputs":[],"source":["# Create a class to process data\n","# import spark ml related libraries\n","class OutcomeCreater(Transformer):\n","    def __init__(self):\n","        super().__init__()\n","        \n","    def _transform(self, df):\n","        df = df.withColumnRenamed(\n","            \"overall\", \"outcome\"\n","        )  # rename the overall column to outcome\n","        return df\n","\n","\n","class ColumnDropper(Transformer):\n","    def __init__(self, cols_to_drop=None):\n","        super().__init__()\n","        self.cols_to_drop = cols_to_drop\n","\n","    def _transform(self, df):\n","        return df.drop(*self.cols_to_drop)\n","\n","\n","class DataPreprocess1(Transformer):\n","    \"\"\"for columns like ls, st..., gk\n","    columns that contains + or - as string\n","    \"\"\"\n","\n","    def __init__(self, cols_to_preprocess) -> None:\n","        super().__init__()\n","        self.cols_to_preprocess = cols_to_preprocess\n","\n","    def _transform(self, df):\n","        from pyspark.sql.functions import split\n","        from pyspark.sql.types import IntegerType\n","\n","        for col in self.cols_to_preprocess:\n","            df = df.withColumn(col, split(\n","                df[col], r'\\+|-').getItem(0).cast(IntegerType()))\n","        return df\n","\n","\n","class DataPreprocess2(Transformer):\n","    \"\"\"\n","    Transforme the columns in Positions to binary columns\n","    \"\"\"\n","\n","    def __init__(self, cols_to_preprocess) -> None:\n","        super().__init__()\n","        self.cols_to_preprocess = cols_to_preprocess\n","\n","    def _transform(self, dataset: DataFrame) -> DataFrame:\n","        from pyspark.sql.functions import split, when, col, array_contains\n","        import itertools\n","        for column in self.cols_to_preprocess:\n","            split_positions = split(dataset[column], ', ')\n","            self.distinct_positions = list(set(list(itertools.chain(\n","                *dataset.select(split_positions.alias('positions')).distinct().rdd.flatMap(lambda x: x).collect()))))\n","            print(self.distinct_positions)\n","            for position in tqdm(self.distinct_positions):\n","                dataset = dataset.withColumn(\n","                    'Position_' + position,\n","                    when(array_contains(split_positions, position), 1).otherwise(0)\n","                )\n","            \n","        dataset = dataset.drop(*self.cols_to_preprocess)\n","        return dataset\n","    \n","class MissingValueModeImputer(Transformer):\n","    def __init__(self, cols_to_impute=None):\n","        super().__init__()\n","        self.cols_to_impute = cols_to_impute\n","\n","    def _transform(self, df):\n","        if not self.cols_to_impute:\n","            return df\n","        for column_name in self.cols_to_impute:\n","            df = self._fill_mode(df, column_name)\n","        return df\n","    def _fill_mode(self, df, col_name):\n","        # Calculate the mode \n","        mode = df.groupBy(col_name).count().orderBy('count', ascending=False).first()[0]\n","        return df.na.fill({col_name: mode})\n","\n","def get_preprocess_pipeline():\n","\n","    # Stage for columns to preprocess2\n","    stage_column_pre1 = DataPreprocess1([\"ls\",\n","                                        \"st\",\n","                                        \"rs\",\n","                                        \"lw\",\n","                                        \"lf\",\n","                                        \"cf\",\n","                                        \"rf\",\n","                                        \"rw\",\n","                                        \"lam\",\n","                                        \"cam\",\n","                                        \"ram\",\n","                                        \"lm\",\n","                                        \"lcm\",\n","                                        \"cm\",\n","                                        \"rcm\",\n","                                        \"rm\",\n","                                        \"lwb\",\n","                                        \"ldm\",\n","                                        \"cdm\",\n","                                        \"rdm\",\n","                                        \"rwb\",\n","                                        \"lb\",\n","                                        \"lcb\",\n","                                        \"cb\",\n","                                        \"rcb\",\n","                                        \"rb\",\n","                                        \"gk\",])\n","    \n","    # Stage for columns to preprocess2\n","    stage_column_pre2 = DataPreprocess2([\"player_positions\"])\n","    \n","    # Stage where nominal columns are handled by imputer\n","    cols_to_impute_nominal = [\"league_name\", \"club_position\"]\n","    stage_missing_handler = MissingValueModeImputer(cols_to_impute=cols_to_impute_nominal)\n","    \n","    # find all cols that contains missing values in continuous_cols\n","    cols_to_imputer_numerical = []\n","    for col in continuous_cols:\n","        if data.select(col).filter(data[col].isNull()).count() > 0:\n","            cols_to_imputer_numerical.append(col)\n","    print(cols_to_imputer_numerical)\n","    \n","    from pyspark.ml.feature import Imputer\n","    stage_missing_handler2 = Imputer(strategy='mean', inputCols=cols_to_imputer_numerical, outputCols=cols_to_imputer_numerical)\n","    \n","    # Stage where nominal columns are transformed  to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_onehot\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(\n","        inputCols=nominal_cols, outputCols=nominal_id_cols)\n","    \n","\n","    \n","    \n","    # Stage where nominal columns are transformed to onehot columns using OneHotEncoder\n","    stage_nominal_onehot = OneHotEncoder(\n","        inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","\n","    # Stage where ordinal columns are transformed to index columns using StringIndexer\n","    ordinal_id_cols = [x+\"_index\" for x in ordinal_cols]\n","    stage_ordinal_indexer = StringIndexer(\n","        inputCols=ordinal_cols, outputCols=ordinal_id_cols)\n","\n","    feature_cols = continuous_cols + position_binary_cols + ordinal_id_cols + nominal_onehot_cols\n","    #feature_cols =  continuous_cols + ordinal_id_cols + nominal_onehot_cols\n","\n","    # Stage where all the features are assembled into a single vector\n","    stage_vector_assembler = VectorAssembler(\n","        inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(\n","        inputCol='vectorized_features', outputCol='features')\n","\n","    # Stage for creating the outcome column representing whether there is attack\n","    stage_outcome = OutcomeCreater()\n","    \n","    # Stage for columns dropping\n","    stage_column_dropper = ColumnDropper(cols_to_drop=cols_to_drop + feature_cols + ordinal_cols + nominal_cols + nominal_id_cols + ['vectorized_features'])\n","    \n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_column_pre1,\n","                                stage_column_pre2,\n","                                stage_missing_handler,\n","                                stage_missing_handler2,\n","                                stage_nominal_indexer,\n","                                stage_nominal_onehot,\n","                                stage_ordinal_indexer,\n","                                stage_vector_assembler,\n","                                stage_scaler,\n","                                stage_outcome,\n","                                stage_column_dropper])\n","    \n","    return pipeline"]},{"cell_type":"code","execution_count":10,"id":"c1ee849a","metadata":{},"outputs":[],"source":["# train utils\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd \n","from pyspark.sql.functions import col\n","from tqdm import tqdm\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_model(model, train_loader, val_loader, epochs, learning_rate):\n","    model = model.to(device)\n","    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in tqdm(train_loader):\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            targets = targets.reshape(-1,1).to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        val_loss = validate_model(model, val_loader, criterion)\n","        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","\n","    return model\n","\n","def validate_model(model, val_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            targets = targets.reshape(-1,1).to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(val_loader)\n","    return avg_loss\n","\n","def data_loader(train, val, batch_size=2048):\n","    # Load your dataset here and preprocess\n","    train_df = train.select('outcome', 'features').toPandas()\n","    val_df = val.select('outcome', 'features').toPandas()\n","    \n","    X_train = np.array(train_df['features'].apply(lambda x: x.toArray()).tolist())\n","    y_train = np.array(train_df['outcome'].values)\n","    X_val = np.array(val_df['features'].apply(lambda x: x.toArray()).tolist())\n","    y_val = np.array(val_df['outcome'].values)\n","\n","    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    return train_loader, val_loader"]},{"cell_type":"code","execution_count":11,"id":"d44c8ff5","metadata":{},"outputs":[],"source":["def load_data(hdfs_data_path):\n","    # merge csvs and read data\n","# fifa data folder should contain all the csv files from Fifa(Kaggle), 2015-2022\n","# assume that you are working in the same directory as the data folder\n","\n","    # Read CSV files from HDFS\n","    csv_files = spark.sparkContext.wholeTextFiles(hdfs_data_path + \"/*.csv\").keys().collect()\n","    print(csv_files)\n","\n","    combined_df = None\n","\n","    for file in tqdm(csv_files):\n","        year = file.split(\"players_\")[1].split(\".csv\")[0]\n","        df = spark.read.csv(file, header=True, inferSchema=True)\n","        df = df.withColumn(\"year\", lit(int(year))) # Add 'year' column\n","        if combined_df is None:\n","            combined_df = df\n","        else:\n","            combined_df = combined_df.union(df)\n","\n","    # Add a unique ID column\n","    combined_df = combined_df.withColumn(\"id\", monotonically_increasing_id())\n","    \n","    return combined_df"]},{"cell_type":"code","execution_count":12,"id":"53fefce1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['hdfs://cluster-14ef-m/fifadata/players_15.csv', 'hdfs://cluster-14ef-m/fifadata/players_17.csv', 'hdfs://cluster-14ef-m/fifadata/players_19.csv', 'hdfs://cluster-14ef-m/fifadata/players_20.csv', 'hdfs://cluster-14ef-m/fifadata/players_16.csv', 'hdfs://cluster-14ef-m/fifadata/players_18.csv', 'hdfs://cluster-14ef-m/fifadata/players_21.csv', 'hdfs://cluster-14ef-m/fifadata/players_22.csv']\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8/8 [00:19<00:00,  2.49s/it]                                   \n"]}],"source":["# HDFS path to 'fifadata' folder\n","hdfs_data_path = \"hdfs:///fifadata\"  # HDFS path\n","data = load_data(hdfs_data_path)"]},{"cell_type":"code","execution_count":13,"id":"ba16b0c5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['value_eur', 'wage_eur', 'club_team_id', 'league_level', 'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic']\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['LB', 'CDM', 'LWB', 'RB', 'CB', 'RW', 'RM', 'RWB', 'GK', 'CAM', 'CF', 'LM', 'LW', 'CM', 'ST']\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 15/15 [00:00<00:00, 32.93it/s]\n","23/11/16 18:52:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['LB', 'CDM', 'LWB', 'RB', 'CB', 'RW', 'RM', 'RWB', 'GK', 'CAM', 'CF', 'LM', 'LW', 'CM', 'ST']\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 15/15 [00:00<00:00, 16.31it/s]\n","                                                                                \r"]},{"data":{"text/plain":["DataFrame[outcome: int, club_joined: timestamp, mentality_composure: string, features: vector]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# get the pipeline \n","pipeline = get_preprocess_pipeline()\n","pipeline_model = pipeline.fit(data)\n","data = pipeline_model.transform(data)\n","\n","train, test = data.randomSplit([0.8, 0.2], seed=SEED)\n","train, val = train.randomSplit([0.75, 0.25], seed=SEED)\n","\n","train.cache()\n","val.cache()\n","test.cache()"]},{"cell_type":"markdown","id":"5cefc9df","metadata":{},"source":["### Training pipeline"]},{"cell_type":"code","execution_count":14,"id":"270f5811","metadata":{},"outputs":[],"source":["from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n","import matplotlib.pyplot as plt\n","class PySparkMLModel:\n","    def __init__(self, model_type=\"logistic\", learning_rate=0.01, is_wandb=False, is_plot=False):\n","        self.model_type = model_type\n","        self.learning_rate = learning_rate\n","        self.model = None\n","        self.evaluator = RegressionEvaluator(labelCol=\"outcome\", predictionCol=\"prediction\", metricName=\"rmse\")\n","        self.wandb = is_wandb\n","        self.is_plot = is_plot\n","\n","        # Initialize Weights & Biases\n","        if self.wandb: \n","            wandb.init(project=\"pyspark_ml_model\", entity=\"your_username\")\n","            wandb.config.update({\"learning_rate\": self.learning_rate})\n","\n","    def train(self, train_data):\n","        if self.model_type == \"linear\":\n","            self.model = LinearRegression(featuresCol=\"features\", labelCol=\"outcome\", regParam=self.learning_rate)\n","        elif self.model_type == \"decision_tree\":\n","            self.model = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"outcome\")\n","        else:\n","            raise ValueError(\"Unsupported model type\")\n","\n","        model = self.model.fit(train_data)\n","        return model\n","\n","    def evaluate(self, model, data, data_type=\"test\"):\n","        predictions = model.transform(data)\n","        metric = self.evaluator.evaluate(predictions)\n","        \n","        print(f\"RMSE on {data_type} data = {metric}\")\n","\n","        # Log metrics to wandb\n","        if self.wandb: \n","            wandb.log({f\"{self.model_type}_{data_type}_rmse\": metric})\n","\n","        # Visualizations and additional logging can be added here as needed\n","        predictions = model.transform(data)\n","        if self.is_plot:\n","            self.plot_residuals(predictions)\n","            self.log_feature_importance(model)\n","        \n","    def plot_residuals(self, predictions):\n","        import seaborn as sns\n","        import matplotlib.pyplot as plt\n","        \n","        # Convert to Pandas DataFrame for easier plotting\n","        predictions_df = predictions.select(\"prediction\", \"outcome\").toPandas()\n","        \n","        # Calculate residuals\n","        predictions_df['residuals'] = predictions_df['outcome'] - predictions_df['prediction']\n","\n","        # Plot residuals\n","        plt.figure(figsize=(10, 6))\n","        sns.residplot(x='prediction', y='residuals', data=predictions_df, lowess=True, \n","                      line_kws={'color': 'red', 'lw': 1})\n","        plt.title('Residuals vs Predicted')\n","        plt.xlabel('Predicted Values')\n","        plt.ylabel('Residuals')\n","        plt.axhline(y=0, color='black', linestyle='--')\n","        plt.show()\n","        if self.wandb:\n","            wandb.log({\"residuals_plot\": wandb.Image(plt)})\n","    \n","    def log_feature_importance(self, model):\n","        import seaborn as sns\n","        import matplotlib.pyplot as plt\n","        \n","        if self.model_type == \"decision_tree\":\n","            # Get feature importances\n","            feature_importances = model.featureImportances.toArray()\n","\n","            # Plot feature importances\n","            plt.figure(figsize=(10, 6))\n","            sns.barplot(x=list(range(len(feature_importances))), y=feature_importances)\n","            plt.title('Feature Importances')\n","            plt.xlabel('Features')\n","            plt.ylabel('Importance')\n","            plt.show()\n","            if self.wandb:\n","                # Log feature importances\n","                wandb.log({\"feature_importances\": feature_importances})\n","                wandb.log({\"feature_importances_plot\": wandb.Image(plt)})\n","\n","\n","    def save_model(self, model, model_path):\n","        model.write().overwrite().save(model_path)\n","        if self.wandb: \n","            wandb.save(model_path)\n","\n","    def close(self):\n","        wandb.finish()\n","\n","    def run_pipeline(self, train, val, test):\n","        model = self.train(train)\n","        self.evaluate(model, val, \"validation\")\n","        self.evaluate(model, test, \"test\")\n","        self.save_model(model, f\"{self.model_type}_model\")\n","        if self.wandb: \n","            self.close()\n"]},{"cell_type":"code","execution_count":15,"id":"6cf157e0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["RMSE on validation data = 1.649044212810787\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["RMSE on test data = 1.6798007229413023\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["RMSE on validation data = 1.8573776345569482\n","RMSE on test data = 1.869825649920477\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Training\n","if IS_SPARKML:\n","    logistic_model = PySparkMLModel(model_type=\"linear\", learning_rate=0.01, is_plot=False)\n","    logistic_model.run_pipeline(train, val, test)\n","\n","    dt_model = PySparkMLModel(model_type=\"decision_tree\", is_plot=False)\n","    dt_model.run_pipeline(train, val, test)"]},{"cell_type":"markdown","id":"52bd6659","metadata":{},"source":["### Pytorch Pipeline"]},{"cell_type":"code","execution_count":16,"id":"b1bdfdc1","metadata":{},"outputs":[],"source":["# Downsample all data for training \n","downsample_train = train.sample(False, 0.1)\n","downsample_val = val.sample(False, 0.1)"]},{"cell_type":"code","execution_count":17,"id":"71584e54","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["8521"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["downsample_train.count()"]},{"cell_type":"code","execution_count":18,"id":"64bcfa29","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Training MLPRegressor model, True\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:03<00:00, 91.36it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Train Loss: 324.0882, Validation Loss: 5.3124\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 146.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Train Loss: 4.5760, Validation Loss: 3.7261\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 133.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Train Loss: 3.1283, Validation Loss: 2.4714\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 136.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Train Loss: 2.1354, Validation Loss: 1.8481\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 130.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Train Loss: 1.6743, Validation Loss: 1.5745\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 137.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Train Loss: 1.4481, Validation Loss: 1.3337\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 130.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Train Loss: 1.2808, Validation Loss: 1.2157\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 133.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Train Loss: 1.1568, Validation Loss: 1.1413\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 136.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Train Loss: 1.0847, Validation Loss: 1.0211\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02<00:00, 126.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Train Loss: 0.9756, Validation Loss: 0.9347\n","Test loss for MLPRegressor model = 0.9346799701452255\n","Training MLPResidualRegressor model, True\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:08<00:00, 37.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Train Loss: 63.2848, Validation Loss: 2.8400\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:08<00:00, 37.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Train Loss: 2.0971, Validation Loss: 1.7931\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Train Loss: 1.5097, Validation Loss: 1.2511\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Train Loss: 1.2923, Validation Loss: 1.4898\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Train Loss: 1.2487, Validation Loss: 0.9873\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Train Loss: 1.2619, Validation Loss: 2.2133\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Train Loss: 1.6741, Validation Loss: 4.1413\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Train Loss: 1.4657, Validation Loss: 1.7551\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 27.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Train Loss: 1.5425, Validation Loss: 1.8964\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:12<00:00, 26.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Train Loss: 1.5406, Validation Loss: 1.0111\n","Test loss for MLPResidualRegressor model = 1.0111230186053686\n"]}],"source":["train_loader_MLP, val_loader_MLP = data_loader(train=train, val=val, batch_size=256)\n","input_dim = train.select('features').first()[0].size\n","model_list = [\n","    MLPRegressor(input_size=input_dim, hidden_sizes=[512, 256, 64], output_size=1),\n","    #TransformerRegressor(input_size=input_dim, d_model=512, nhead=8, num_layers=6, output_size=1)\n","    MLPResidualRegressor(input_size=input_dim, hidden_sizes=[1024,512,256,64], output_size=1)\n","    ]\n","\n","for model in model_list:\n","    if model.__class__.__name__ == 'MLPRegressor':\n","        print(f\"Training {model.__class__.__name__} model, {model.__class__.__name__ == 'MLPRegressor'}\")\n","        trained_model = train_model(model, train_loader_MLP, val_loader_MLP, epochs=10, learning_rate=0.0003)\n","        torch.save(trained_model.state_dict(), f\"{model.__class__.__name__}.pt\")\n","        trained_model.eval()\n","        test_loss = validate_model(model, val_loader_MLP, nn.MSELoss())\n","        print(f\"Test loss for {model.__class__.__name__} model = {test_loss}\")\n","    else: \n","        print(f\"Training {model.__class__.__name__} model, {model.__class__.__name__ == 'MLPResidualRegressor'}\")\n","        trained_model = train_model(model, train_loader_MLP, val_loader_MLP, epochs=10, learning_rate=0.001)\n","        torch.save(trained_model.state_dict(), f\"{model.__class__.__name__}.pt\")\n","        trained_model.eval()\n","        test_loss = validate_model(model, val_loader_MLP, nn.MSELoss())\n","        print(f\"Test loss for {model.__class__.__name__} model = {test_loss}\")\n","    del trained_model \n","    #torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":19,"id":"9b2aef4f","metadata":{},"outputs":[],"source":["spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
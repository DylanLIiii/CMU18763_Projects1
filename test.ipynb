{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pandas\n",
    "# Reducing dataframe memory usage :-\n",
    "def ReduceMemory(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function reduces the associated dataframe's memory usage.\n",
    "    It reassigns the data-types of columns according to their min-max values.\n",
    "    It also displays the dataframe information after memory reduction.\n",
    "    \"\"\";\n",
    "    \n",
    "    # Reducing float column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('float').columns):\n",
    "        col_min = np.amin(df[col].dropna());\n",
    "        col_max = np.amax(df[col].dropna());\n",
    "        \n",
    "        if col_min >= np.finfo(np.float16).min and col_max <= np.finfo(np.float16).max: \n",
    "            df[col] = df[col].astype(np.float16)\n",
    "        elif col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max : \n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        else: pass;\n",
    "\n",
    "    # Reducing integer column memory usage:-\n",
    "    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('int').columns):\n",
    "        col_min = df[col].min(); \n",
    "        col_max = df[col].max();\n",
    "        \n",
    "        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8);\n",
    "        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16);\n",
    "        elif col_min >= np.iinfo(np.int32).min & col_max <= np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32);\n",
    "        else: pass;\n",
    "        \n",
    "    display(df.info()); \n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/fifadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files: \n",
    "    df = pd.read_csv(file)\n",
    "    df['year'] = int(f'20{file[-6:-4]}')\n",
    "    df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in csv_files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ReduceMemory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the concatenated DataFrame to a new CSV file\n",
    "output_file = '/Users/dylan/DylanLi/Code_Repo/CMU18763_Projects1/full_data.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read table and write to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "appName = \"Project1\"\n",
    "master = \"local\"\n",
    "\n",
    "\n",
    "sc = SparkSession.builder.appName(appName).getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read multiple csv files into spark dataframe\n",
    "df = spark.read.csv(output_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert unique id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"010323\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "    \n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1 = spark.sql(\"\"\"\n",
    "SELECT dv.club_name, COUNT(*) AS player_count\n",
    "FROM   df_view dv\n",
    "WHERE  dv.year = 2022 AND dv.club_contract_valid_until = 2023\n",
    "GROUP BY dv.club_name\n",
    "ORDER BY player_count DESC\n",
    "LIMIT 5;\n",
    "\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2 = spark.sql(\"\"\"\n",
    "WITH club_counts AS (\n",
    "    SELECT dv.club_name, dv.year, COUNT(*) AS player_count\n",
    "    FROM df_view dv\n",
    "    WHERE dv.age > 27\n",
    "    GROUP BY dv.club_name, dv.year\n",
    "),\n",
    "club_averages AS (\n",
    "    SELECT cc.club_name, AVG(cc.player_count) AS average_count\n",
    "    FROM club_counts cc\n",
    "    GROUP BY cc.club_name\n",
    ")\n",
    "SELECT ca.club_name, ca.average_count\n",
    "FROM club_averages ca\n",
    "WHERE (\n",
    "    SELECT COUNT(*) FROM club_averages ca2 WHERE ca2.average_count > ca.average_count\n",
    ") < 10\n",
    "ORDER BY ca.average_count DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3 = spark.sql(\"\"\"\n",
    "WITH yearly_counts AS (\n",
    "    SELECT dv.year, dv.nation_position, COUNT(*) AS position_count\n",
    "    FROM df_view dv\n",
    "    WHERE dv.nation_position IS NOT NULL\n",
    "    GROUP BY dv.year, dv.nation_position\n",
    "),\n",
    "max_counts AS (\n",
    "    SELECT yc.year, MAX(yc.position_count) AS max_count\n",
    "    FROM yearly_counts yc\n",
    "    GROUP BY yc.year\n",
    ")\n",
    "SELECT mc.year, yc.nation_position, mc.max_count\n",
    "FROM max_counts mc\n",
    "JOIN yearly_counts yc ON mc.year = yc.year AND mc.max_count = yc.position_count\n",
    "ORDER BY mc.year;\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Useless column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless column\n",
    "useless_columns = ['sofifa_id', 'player_url', 'long_name', 'dob', 'club_loaned_from',\n",
    "                   'nation_position', 'nation_jersey_number', 'body_type', 'real_face',\n",
    "                   'player_face_url', 'club_logo_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                    'goalkeeping_speed', 'player_tags', 'nation_team_id', 'short_name', 'league_name','id', 'club_joined','club_contract_valid_until'] #TODO how to deal with time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in linux use\n",
    "# df_read = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = df_read.drop(*useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete columns that include url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get a list of columns that include URLs\n",
    "url_columns = [c for c in new_df.columns if 'url' in c]\n",
    "\n",
    "# Drop the columns that include URLs\n",
    "new_df = new_df.drop(*url_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Columns that Missing Value are more than 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in new_df.columns:\n",
    "    missing = new_df.filter(col(i).isNull()).count() / new_df.count() * 100\n",
    "    if missing > 50:\n",
    "        print('{} - {}%'.format(i, round(missing)))\n",
    "        cols_to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop value after +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "columns1 = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram',\n",
    "            'lm','lcm','cm','rcm','rm','lwb','ldm', 'cdm','rdm','rwb',\n",
    "            'lb','lcb','cb','rcb','rb']\n",
    "\n",
    "for col in columns1:\n",
    "    new_df = new_df.withColumn(col, split(new_df[col], '\\+').getItem(0).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_value = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [c for c, t in new_df.dtypes if t == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in string_cols:\n",
    "  new_df = new_df.fillna(na_value, subset=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, when, col, array_contains\n",
    "import itertools\n",
    "\n",
    "# Split positions into array\n",
    "split_positions = split(new_df['player_positions'], ', ')  \n",
    "\n",
    "# Get distinct positions as a list\n",
    "distinct_positions = list(set(list(itertools.chain(*new_df.select(split_positions.alias('positions')).distinct().rdd.flatMap(lambda x: x).collect()))))\n",
    "\n",
    "# Create a column for each distinct position\n",
    "for position in distinct_positions:\n",
    "  new_df = new_df.withColumn(\n",
    "    'Position_' + position,\n",
    "     when(array_contains(split_positions, position), 1).otherwise(0)\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop('player_positions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values again\n",
    "na_counts = new_df.select([_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in new_df.columns])\n",
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna preprocess the preffered_foot using one-hot encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "indexer = StringIndexer(inputCol='preferred_foot', outputCol='indexed_preferred_foot')\n",
    "encoder = OneHotEncoder(inputCols=['indexed_preferred_foot'], outputCols=['preferred_foot_encoded'])\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer,encoder])\n",
    "\n",
    "model = pipeline.fit(new_df)\n",
    "\n",
    "col_to_drop = ['indexed_preferred_foot','preferred_foot']\n",
    "data_encoded = model.transform(new_df).drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder for work_rate and player_positions label_encoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Loop over each string column in the DataFrame\n",
    "for col_name, data_type in data_encoded.dtypes:\n",
    "    if data_type == 'string':\n",
    "        # Create a StringIndexer object and fit it to the column\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=col_name + '_indexed')\n",
    "        model = indexer.fit(data_encoded)\n",
    "        \n",
    "        # Transform the column using the fitted indexer\n",
    "        data_encoded = model.transform(data_encoded).drop(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aichaintools-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
